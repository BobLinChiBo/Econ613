\documentclass{beamer}
\setbeamertemplate{navigation symbols}{}
\usepackage[latin1]{inputenc}
\usepackage{setspace,dsfont}
\usepackage{amsmath,amssymb,pdfpages}
\usepackage[longnamesfirst,nonamebreak]{natbib}
\usepackage[english]{babel}
\usepackage{eurosym,multirow,hyperref,cmll}
\usepackage{listings}

\newcommand{\Lik}{\mathcal{L}}
\newcommand{\lau}{\lambda_u}
\newcommand{\wi}{\underline{w}}
\newcommand{\m}{\mathcal{M}}
\newcommand{\wa}{\overline{w}}
\newcommand{\lae}{\lambda_e}
\newcommand{\1}{\mathbb{1}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\f}{\mathfrak{f}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\hy}{\hat{y}}

\hypersetup{
    colorlinks,%
    citecolor=blue,%
    filecolor=blue,%
    linkcolor=blue,%
    urlcolor=blue,
}
%\usetheme{Boadilla}
%\usetheme{Marburg}
%\usetheme{Hannover}
%\usetheme{Pittsburgh}
%\usetheme{umbc1}
%\usetheme{Montpellier}
%\usetheme{Singapore}
%\usetheme{}
%\usetheme{}
%\lstset {language=C++}

\DeclareMathOperator{\plim}{plim}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\beamersetuncovermixins{\opaqueness<1>{25}}{\opaqueness<2->{15}}
\begin{document}
\begin{frame}
\title{ECON 613: Applied Econometrics}
\subtitle{Methods}
\titlepage
\end{frame}

\frame{\frametitle{Overview: Linear Models}
\begin{itemize}
\item Study the relationship between an outcome variable $y$ and a set of regressors $x$. 
\begin{itemize}
\item Conditional Prediction.
\item Causal inference.
\item Example: propensity to consume. 
\end{itemize}
\item Loss function approach
\begin{equation*}
L(e) =  L(y-\hy)
\end{equation*}
where $\hy = E(y\mid x) $ is a predictor of y, and the error $e = y-\hat{y}$
\end{itemize}
}

\frame{\frametitle{Squared Loss Function}
\begin{itemize}
\item Squared error loss: $L(e) = e^2$
\item Optimization problem
\begin{equation*}
\min_{\beta} \sum_{i}^N (y_i - f(x_i,\beta))^2
\end{equation*}
\end{itemize}
}


\frame{\frametitle{Linear Prediction}
\begin{itemize}
\item $E[y \mid x] = x'\beta$
\item OLS
\begin{equation*}
y = x\beta + e
\end{equation*}
\item Derivation 
\begin{eqnarray*}
L(\beta) &=& (y - x\beta)'(y-x\beta) \\
     &=&  y'y - 2y'x\beta + \beta' X'X\beta
\end{eqnarray*}
Then 
\begin{equation*}
\dfrac{\partial L(\beta)}{\partial \beta} = - 2x'y + 2x'x\beta =0
\end{equation*}
\item Formula 
\begin{equation*}
\hat{\beta} = (x'x)^{-1}x'y
\end{equation*}
\end{itemize}
}

\frame{\frametitle{Properties}
see 4.4.4 and 4.4.5. 
}

\frame{\frametitle{Properties of an estimator}
\begin{itemize}
\item Unbiasedness: $E(\hat{\theta}) = \theta$.
\item Consistency: $plim \hat{\theta}_n = \theta$.
\item Efficiency: Reach Cramer-Rao lower bound asymptotically. 
\end{itemize}
}

\defverbatim[colored]\lstI{
\begin{lstlisting}[language=R,basicstyle=\ttfamily,keywordstyle=\color{red}]
////R 
fitR = lm(Y~X)

////Matlab
fitM = fitlm(X,Y,'linear')

////Stata
fitM = reg Y X
\end{lstlisting}
}

\frame{\frametitle{Codes}
\lstI
}

\defverbatim[colored]\lstR{
\begin{lstlisting}[language=R,basicstyle=\ttfamily,keywordstyle=\color{red}]
////R 
 
####  define X matrix and y vector
X = as.matrix(cbind(1,X))
y = as.matrix(Y)
 
####  estimate the coeficients beta
####  beta = ((X'X)^(-1))X'y
beta  = solve(t(X)%*%X)%*%t(X)%*%y
\end{lstlisting}
}

\frame{\frametitle{Codes}
\lstR
}

\section{Maximum Likelihood}

\begin{frame}
\tableofcontents[currentsection] 
\end{frame}

\frame{\frametitle{Introduction to MLE}
Consider a parametric model in which the joint distribution of $Y=(Y_1,\ldots,Y_n)$ has a density $\ell(y,\theta)$ with respect to a measure $\mu$. Then consider $P_{\theta}=\ell(y,\theta) \mu$ where $\theta \in \Theta \in \Real^p$. Once $y=(y_1,\ldots,y_n)$ is observed, the maximum likelihood method consists of estimating the parameter $\theta$ a value $\hat{\theta}(y)$ that maximizes the likelihood function $\theta \rightarrow \ell(y,\theta)$. Formally, a maximum likelihood estimator of $\theta$ is a solution to the maximization problem 
\begin{equation*}
\max_{\theta} \ell(Y;\theta) 
\end{equation*}
or
\begin{equation*}
\max_{\theta} \log(\ell(Y;\theta))
\end{equation*}
}

\begin{frame}\frametitle{Feasible examples: Poisson distribution}
Consider a dependent variable that takes only non negative integer values $0,1,2,\ldots$, and one assumes that the dependent variable follows a Poisson distribution, and we wishes to estimate the Poisson parameter. 
\begin{itemize}
\item Given $y_i \sim f(\lambda,y_i) = \frac{\exp(-\lambda) \lambda^{y_i}}{y_i !}$
\item Likelihood $\Lik(y; \lambda)= \prod_{i=1}^{N}   \frac{\exp(-\lambda) \lambda^{y_i}}{y_i !} = \frac{\exp(-N\lambda) \lambda^{\sum_i^N y_i}}{ \prod_{i=1}^{N}  y_i !}$
\item Log likelihood $\log \Lik(y; \lambda) = - N \lambda + \sum_i^N y_i \log(\lambda) - \sum_i^N \log(y_i !)$
\item Estimate \begin{equation*}
\frac{\partial \log \Lik(y; \lambda)}{\partial \lambda} = 0 \Longrightarrow \widehat{\lambda} = \frac{\sum_i^N y_i}{N}
\end{equation*}
\end{itemize}
\end{frame}


\frame{\frametitle{Feasible examples: Least Squares}
\begin{itemize}
\item Normality assumption $e \sim \N(0,\sigma^2)$, then $y \sim \N(x\beta,\sigma^2)$.
\item Likelihood $L(\beta) = (2 \pi \sigma^2)^{-\tfrac{N}{2}} \exp(-0.5 \sigma^{-2} (y-x\beta)'(y-x\beta))$ 
\item log likelihood $\log L(\beta) = -\tfrac{N}{2}\log \sigma^2 - \tfrac{1}{2\sigma^2} (y-x\beta)'(y-x\beta)$
\item $\beta = (x'x)^{-1}x'y$
\end{itemize}
}

\frame{\frametitle{Some difficulties}
\begin{itemize}
\item Non-uniqueness of the Likelihood Function 
\item Non-existence of a solution to the Maximization Problem
\item Multiple Solutions to the Maximization Problem
\end{itemize}
}

\frame{\frametitle{Asymptotic Properties (1): Convergence}
\begin{definition}
Under a set of regularity conditions, there exists a sequence of maximum likelihood estimators converging almost surely to the true parameter value $\theta_0$
\end{definition}
\begin{itemize}
\item The variables $Y_i,i=1,2,\ldots$ are independent and identically distributed with density $f(y;\theta), \theta \in \Theta \in \Real^p$
\item The parameter space $\Theta$ is compact.
\item The log likelihood function  $\Lik(y,\theta)$ is continuous in $\theta$ and is a measurable function of $y$. 
\item The log-likelihood function is such that $(1/n)\Lik_n(y,\theta)$ converges surely to $E_{\theta_0} log(f(Y_i;\theta))$ uniformly in $\theta \in \Theta$. $E_{\theta_0} log(f(Y_i;\theta))$ exists. 
\end{itemize}
}

\frame{\frametitle{Asymptotic Properties (2): Asymptotic Normality}
\begin{itemize}
\item The log likelihood function $\Lik_n(\theta)$ is twice continuously differentiable in an open neighborhood of $\theta_0$
\item The matrix (Fisher Information Matrix) $\mathcal{I}_1(\theta_0) = E_{\theta_0}  \left( - \dfrac{\partial^2 \log f(Y_1;\theta_0)}{\partial \theta \partial \theta'} \right)$
\end{itemize}
\begin{definition}
\begin{equation*}
\sqrt{n}(\hat{\theta}_n - \theta_0) \rightarrow \N(0,\mathcal{I}_1(\theta_0)^{-1}).
\end{equation*}
\end{definition}
}

\frame{\frametitle{Concentrated Likelihood Function}
\begin{definition}
Let the parameter set $\theta = (\alpha,\beta)$. The solutions $\hat{\theta} = (\hat{\alpha},\hat{\beta})$ to the mazimization problem $\max_{\alpha,\beta} \log \Lik(y;\alpha,\beta)$ can be obtained via the following two-step procedure:
\begin{description}
\item[a)] Maximize the log-likelihood function with respect $\alpha$ given $\beta$. The maximum value is attained for values of $\alpha$ in a set $A(\beta)$ depending on the parameter $\beta$. Thus, if $\alpha \in A(\beta)$, the log-likelihood value is 
\begin{equation*}
\log \Lik_c(y;\beta) = \max_{\alpha} \log \Lik(y;\alpha,\beta)
\end{equation*}
The mapping $\log \Lik_c$ is called the concentrated (in $\alpha$) log likelihood function.
\item[b)] In a second step, maximize the concentrated log-likelihood function with respect to $\beta$. 
\end{description}
\end{definition}
}

\frame{\frametitle{Application}
Consider the likelihood
\begin{equation*}
\Lik(y,\beta,\sigma) = - \tfrac{n}{2} \log 2\pi - \tfrac{n}{2} \log \sigma^2 -\tfrac{1}{2 \sigma^2} (y-x\beta)'(y-x\beta)
\end{equation*}
\begin{itemize}
\item First step
\begin{equation*}
\dfrac{\partial \Lik(y;\beta,\sigma)}{\partial \sigma} = - \tfrac{n}{\sigma} + \tfrac{1}{\sigma^3} (y-x\beta)'(y-x\beta) = 0
\end{equation*}
Then 
\begin{equation*}
\sigma^2(\beta) =  \tfrac{1}{n} (y-x\beta)'(y-x\beta)
\end{equation*}
\item Substituting $\sigma^2(\beta)$ into the likelihood
\begin{equation*}
\Lik_c(y,\beta,\sigma) = - \tfrac{n}{2} \log 2\pi - \tfrac{n}{2} \log \tfrac{1}{n} (y-x\beta)'(y-x\beta) - \tfrac{n}{2}
\end{equation*}
\end{itemize}
}

\begin{frame}\frametitle{Hypothesis Testing}
Three procedures to do tests
\end{frame}

\begin{frame}\frametitle{Likelihood Ratio}
\begin{itemize}
\item The likelihood ratio statistic is 
\begin{equation*}
LR = 2(\ell(\theta) - \ell(\tilde{\theta}))
\end{equation*}

where $\hat{\theta}$ and $\tilde{\theta}$ are the restricted and unrestricted maximum likelihood estimates of $\theta$. 
\item Wilk's theorem shows that 

\begin{equation*}
LR \sim \chi^2(r)
\end{equation*}
where $r$ is the number of restrictions. 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Additional Tests}
\begin{itemize}
\item Wald Test
\item LM test
\end{itemize}
We will see in GMM.
\end{frame}


\begin{frame}\frametitle{In practice}
\begin{itemize}
\item The regularity conditions are strong. 
\item What happens if we weaken them?
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Problem 1}
Number of parameters increases with the number of observations
\begin{itemize}
\item Convergence holds
\item Estimates may be biased
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Problem 2}
True parameter value $\theta_0$ does not belong to $\Theta$: The model is misspecified
\begin{itemize}
\item Convergence holds to a parameter that is not the true parameter.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Problem 3}
Correlated Observations
\begin{itemize}
\item Convergence does not hold. 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Problem 4}
Discontinuity of the likelihood function
\begin{itemize}
\item Numerical problems.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Problem 5}
Known parameter space
\begin{itemize}
\item Constrained Optimization
\end{itemize}
\end{frame}

\section{GMM}

\begin{frame}
\tableofcontents[currentsection] 
\end{frame}

\begin{frame}\frametitle{Method of Moments}
\begin{itemize}
\item Orthogonality condition in Linear Models
\begin{equation}
E(x(y'-x)) = 0
\end{equation}
\item Moment Condition
\begin{equation}
\dfrac{1}{N} \sum_i x_i(y_i - x'\beta)
\end{equation}
\item Moment Estimator
\begin{equation}
\hat{\beta}_{\text{MM}} = (\sum_i x_i x'_i)^{-1}(\sum_i x_i y_i)
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Nonlinear Model}
\begin{itemize}
\item Consider 
\begin{equation*}
Y_i = g(X_i,b_0) + u_i 
\end{equation*}
\item Orthogonality Condition
\begin{equation*}
E[X'(y-g(X,b_0))] = 0 
\end{equation*}
\item Moments condition
\begin{equation*}
E_0 h(Y,X,a_0) = 0
\end{equation*}
\item The function $h$ is H-dimensional and the parameter $a$ is of size $K$.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Formal Idea}
\begin{definition}
The basic idea of generalized method of moments is to choose a value for $a$ such that the sample mean is closest to zero.
\begin{equation*}
\dfrac{1}{n} \sum_{i=1}^{n} h(Y_i,X_i,a)
\end{equation*}
\end{definition}
\end{frame}

\begin{frame}\frametitle{Formal Definition}
\begin{definition}
Let $\mathbb{S}_n$ be an $(H \times H)$ symmetric positive definite matrix that may depend on the observations. The generalized method of moments (GMM) estimator associated with $\mathbb{S}_n$ is a solution $\tilde{a}_n(\mathbb{S}_n)$ to the problem
\begin{equation*}
min_a \left[ \sum_{i=1}^{n} h(Y_i,X_i,a) \right]^{'} \mathbb{S}_n \left[ \sum_{i=1}^{n} h(Y_i,X_i,a) \right]
\end{equation*}
\end{definition}
\end{frame}

\begin{frame}\frametitle{Assumptions}
\begin{description}
\item[H1] The variables $(Y_i,X_i)$ are independent and identically distributed.
\item[H2] The expectation $E_0 h(Y,X,a)$ exists and is zero when $a$ is equal to the true value $a_0$ of the parameter of interest.
\item[H3] The matrix $\mathbb{S}_n$ converges almost surely to a nonrandom matrix $\mathbb{S}_0$
\item[H4] The parameter $a_0$ is identified from the equality constraints, i.e. $E_0 h(Y,X,a)' \mathbb{S}_0 E_0 h(Y,X,a) = 0$
\item[H5] The parameter value $a_0$ is known to belong to a compact set $\mathcal{A}$
\item[H6] The quantity $(1/n)\sum_{i=1}^n h(Y_i,X_i,a)$ converges almost surely and uniformly in $a$ to $E_0 h(Y,X,a)$
\item[H7] The function $h(Y,X,a)$ is continuous in a
\item[H8] The matrix $\left[ E_0\dfrac{h(Y,X,a)}{\partial a}  \right]^{'} \mathbb{S}_0 \left[ E_0\dfrac{h(Y,X,a)}{\partial a'} \right]$ is nonsingular, which implies $H \geq K$.
\end{description}
\end{frame}

\begin{frame}\frametitle{Asymptotic Normality}
Under the assumptions, we have
\begin{equation*}
\sqrt{n}(\tilde{a}_n(\mathbb{S}_n) - a_0) \sim \N(0,\Sigma(\mathbb{S}_0))
\end{equation*}

where 
\begin{align*}
\Sigma(\mathbb{S}_0) = \left(\left[ E_0\dfrac{h(Y,X,a)}{\partial a}  \right]^{'} \mathbb{S}_0 \left[ E_0\dfrac{h(Y,X,a)}{\partial a'} \right] \right)^{-1}\\
\left(\left[ E_0\dfrac{h(Y,X,a)}{\partial a}  \right]^{'} \mathbb{S}_0 V_0(h(Y,X,a_0)) \mathbb{S}_0 \left[ E_0\dfrac{h(Y,X,a)}{\partial a'} \right] \right)^{-1}\\
\left(\left[ E_0\dfrac{h(Y,X,a)}{\partial a}  \right]^{'} \mathbb{S}_0 \left[ E_0\dfrac{h(Y,X,a)}{\partial a'} \right] \right)^{-1}
\end{align*}

\end{frame}

\begin{frame}\frametitle{Optimal GMM}
\begin{itemize}
\item $\mathbb{S}_0$ is not known.
\item Two-step procedure
\begin{itemize}
\item Estimate 
\begin{equation*}
min_a \left[ \sum_{i=1}^{n} h(Y_i,X_i,a) \right]^{'} I \left[ \sum_{i=1}^{n} h(Y_i,X_i,a) \right]
\end{equation*}
where $I$ is the identity matrix, and recover $\hat{a}$.
\item Matrix of variance/covariance
\begin{equation*}
\hat{\mathbb{S}} = \dfrac{1}{N} \sum_{i=1}^{n} h(Y_i,X_i,\hat{a}) h(Y_i,X_i,\hat{a})'
\end{equation*}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Relationship to IV.}
\begin{itemize}
\item Nonlinear 2SLS is a very good application of GMM.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Inference}
\begin{itemize}
\item Over identification test see iv section.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Applications}
\begin{itemize}
\item Matrix of variance/covariance in practice
\item Indirect Inference
\item Simulated method of moments
\end{itemize}
\end{frame}

\section{Numerical Optimization}

\begin{frame}
\tableofcontents[currentsection] 
\end{frame}

\begin{frame}\frametitle{Numerical Optimization}
Most maximum likelihood estimates require numerical optimization. 
\end{frame}


\begin{frame}\frametitle{Primer on optimization}
\textbf{Definition}\\
\begin{equation*}
\min_x f(x)
\end{equation*}
\begin{itemize}
\item $x \in \Real^n$
\item $f$ is a smooth function. 
\end{itemize}
\textbf{Existence: Weierstrass theorem}\\
A point or a vector $x^*$ is a global minimizer if $f(x^*) \leq f(x) \, \forall \, x$. 
\end{frame}

\begin{frame}\frametitle{Maximization Vs Minimization}
Let $-f$ denote the function whose value at any value at any x is $-f(x)$. Then,
\begin{enumerate}
\item $x$ is the maximum of $f$ if and only if x is a minimum of $-f$
\item $z$ is a minimum of $f$ if and only if $z$ is a maximum of $-f$ 
\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Necessary conditions}
\begin{enumerate}
\item If $x^{*}$ is local minimizer and $f$ is continuously differentiable in an open neighborhood of $x^{*}$, then $\nabla f(x^*)=0$
\item If $x^{*}$ is local minimizer and $\nabla^2 f$ exists and is continuous in an open neighborhood of $x^{*}$, then $\nabla f(x^*)=0$ and $\nabla^2 f(x^*) = 0$
\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Likelihood setup}
The likelihood function is defined by:
\begin{equation*}
\Lik_n(\theta) = \prod_{i=1}^{n} f(X_i;\theta)
\end{equation*}
The necessary conditions for optimization yield the regularity conditions
\end{frame}

\begin{frame}\frametitle{Unfeasible example: }
\centering \textbf{Any nonlinear model}
\end{frame}

\begin{frame}\frametitle{Numerical optimization}
\begin{itemize}
\item Local optimization: the best minimum/maximum in a vicinity - usually defined by a convergence criteria.
\item Global optimization: Best of all local minimas/maximas.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Numerical optimization - Local Optimization}
Overview
\begin{enumerate}
\item \textbf{Line Search}: Starting from an initial value, choose a direction and search along this direction to find a new iterate  
\item \textbf{Trust region}: Use previous estimates of the objective function, to construct a \textbf{synthetic} or \textbf{model} function whose behavior near the current point is similar to the objective function, and search only over a region, \textit{trust region}, with the underlying idea that the model function is a good approximate over the trust region. 
\end{enumerate}
\end{frame}


\begin{frame}\frametitle{Line Search}
Idea:

\begin{equation*}
x_{k+1} = x_k + \alpha_k d_k 
\end{equation*}

where $d_k$ is a direction to be evaluated, and $\alpha_k$ a scaling parameter. 

The variants of numerical optimization 
\begin{enumerate}
\item Steepest descent: $d_k = - \nabla f(x_k)$
\item Newton direction: $d_k = - (\nabla^2 f(x_k))^{-1}\nabla f(x_k)$
\item Quasi-Newton direction: $d_k = - (B(x_k))^{-1}\nabla f(x_k)$
\item Derivative free.
\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Properties}
\begin{itemize}
\item Robustness: Perform well for various problems and starting values.
\item Efficiency:  
\item Accuracy: Identify a solution with precision, not sensitive to starting values
\end{itemize}
\end{frame}

\frame{\frametitle{One parameter optimization}
\begin{itemize}
\item Bissection
\item Secant Method
\end{itemize}
}

\defverbatim[colored]\bissect{
\begin{lstlisting}[language=R,basicstyle=\ttfamily,keywordstyle=\color{red}]
bisection <- function(f, a, b, n , tol ) {
  # Check the signs of the function.
  if (!(f(a) < 0) && (f(b) > 0)) {
    stop()} else if ((f(a) > 0) && (f(b) < 0)) {
    stop()}
  for (i in 1:n) {
    c <- (a + b) / 2 # Calculate midpoint
    # If the function equals 0 at the midpoint
    if ((f(c) == 0) || ((b - a) / 2) < tol) {
      return(c)  }
    # If another iteration is required, 
    # check the signs of the function
    ifelse(sign(f(c)) == sign(f(a)), 
           a <- c,
           b <- c)
  }
  # If the max number of iterations is reached
  print('Too many iterations')
}
\end{lstlisting}
}

\frame{\frametitle{Codes}
\begin{footnotesize}
\bissect
\end{footnotesize}
}


\begin{frame}\frametitle{Recover the scaling parameter}

\begin{itemize}
\item Solve the function $\phi(\alpha) = f(x_k + \alpha d_k)$
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Quasi-Newton methods}
How to approximate the hessian such that:
\begin{itemize}
\item Reduce the computation time (Use only gradient instead of hessian) 
\item Increase convergence rate 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Conjugate Gradient- FR}
\begin{itemize}
\item Given x0
\item Evaluate $f_0 = f(x0)$, $\nabla f_0 = \nabla f(x_0)$
\item Set $d_0 = - \nabla f_0$, $k=0$
\item \textbf{While} $\nabla f_k \neq 0$
\begin{itemize}
\item Set $x_{k+1} = x_k + \alpha_k d_k$
\item Evaluate $\nabla f_{k+1}$, then:
\begin{itemize}
\item $\beta^{FR}_{k+1} = \dfrac{\nabla f_{k+1}' \nabla f_{k+1}}{\nabla f_{k}'\nabla f_{k}}$
\item $d_{k+1} = -\nabla f_{k+1} + \beta^{FR}_{k+1} d_k$
\item $k = k+1$
\end{itemize}
\end{itemize}
\item \textbf{end(while)}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{BFGS}
\begin{itemize}
\item Given x0
\item Evaluate $f_0 = f(x0)$, $\nabla f_0 = \nabla f(x_0)$, $H_0=I$
\item Set $k=0$
\item \textbf{While} $||\nabla f_k || >\epsilon$
\begin{itemize}
\item Compute direction $d_k = -H_k\nabla f_k$
\item Set $x_{k+1} = x_k + \alpha_k d_k$
\item Evaluate $\nabla f_{k+1}$, then:
{\large
\begin{itemize}
\item set $s_k = x_{k+1}-x_k$, $y_k=\nabla f_{k+1}-\nabla f_k$ and $\rho_k= \dfrac{1}{y_k's_k}$
\item Update $H_{k+1} = (I-\rho_k s_k y_k')H_k(I-\rho_k y_k s_k') + \rho_k s_k s_k'$
\end{itemize}}
\end{itemize}
\item \textbf{end(while)}
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Problems}
\begin{itemize}
\item non differentiable functions
\item disconnected and non-convex feasible space
\item discrete feasible space
\item large dimensionality
\item multiple local minimas
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Derivative Free}
\begin{itemize}
\item Nelder-Mead
\item Simulated annealing
\item Divided Rectangles Method
\item Genetic Algorithms
\item Particle Optimization
\end{itemize}
\end{frame}


\section{Inference}

\begin{frame}
\tableofcontents[currentsection] 
\end{frame}

\begin{frame}\frametitle{How to get standard error}
\begin{itemize}
\item Fisher Information Matrix
\item Sandwich formula
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Introduction to Bootstrap}
\begin{itemize}
\item Inference for small samples basically...
\item Inference for unknown distribution...
\item Computer intensive resampling method...
\begin{itemize}
\item Using data to generate new data
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Applications: Inference in estimation}
\begin{itemize}
\item Bootstrap samples
\item Parallel implementation
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Applications: Inference post estimation}
\begin{itemize}
\item Bootstrap samples
\item Marginal effects
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Applications: Testing}
\begin{itemize}
\item Bootstrap samples
\item Approach to testing
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Applications: Choosing the number of R}
\begin{itemize}
\item Objectives and Constraints
\item R=49,99,199,499,999,9999....
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Delta Method}
\begin{itemize}
\item Consider $ X \sim \N(\mu,\sigma^2)$, and assume you are interested in $E(g(X))$ and $Var(g(X)$
\item Approximation 
\begin{equation}
g(x) = g(\mu) + g'(\mu)(x-\mu)
\end{equation}
and then
\begin{equation}
E(g(x)) \approx g(E(x))
\end{equation}

\begin{equation}
Var(g(x)) \approx g'(E(x))^2 Var(x)
\end{equation}
\item Usage?
\end{itemize}
\end{frame}


\end{document}


